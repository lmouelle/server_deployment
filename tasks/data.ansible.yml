---

# TODO: Containerize restic, so I can just use podman secrets flow everywhere
# Make systemd timer but make the associated service for the timer a quadlet .service file
# The quadlet's entry point is restic check, restic backup or restic forget
# Mount data dir readonly, with small z perms and noexec
# Have my timer run each entrypoint as a diff .service/quadlet that run sequentially, depending on prev success?
# Or... my timer triggers a backup_script.service, which runs podman exec ... $entrypoint within the script?
# Former is more complicated and gives me nothing but failures for distinct stages in systemctl --failed
# Also a second timer to check if proton/rclone config is stale and needs to be refreshed. Or refresh unconditionally every N hours?

# Make all users system users this time with no login shell and a home dir on /data drive?
# Then make this backup service in the data user just backup the entire /data drive, including their home dirs
# which will include any state. Some secrets will be in the backups (deluge state includes auth file as is) but that's fine,
# the backups are encrypted.

- name: Create rclone config for proton remote backend
  become: true
  vars_files: secrets/secret_ids.ansible.yml
  become_user: "{{ username }}"
  environment:
    RCLONE_PROTONDRIVE_USERNAME: "{{ lookup('bitwarden.secrets.lookup', proton_passwd) }}"
    RCLONE_PROTONDRIVE_PASSWORD: "{{ lookup('bitwarden.secrets.lookup', proton_username) }}"
    RCLONE_PROTONDRIVE_2FA: "{{ lookup('bitwarden.secrets.lookup', 'fb93e5ea-23b0-4e31-8d61-b18a004b8e77') }}"
  ansible.builtin.command:
    creates: ~/.config/rclone/rclone.conf
    cmd: "rclone config create --non-interactive {{ backup_remote_name }} protondrive"

- name: Ensure backup container
  become: true
  become_user: torrents
  vars:
    configs:
      backup_data:
        entrypoint: "backup {{ data_dir }} -v"
        quadlet_opt: "Before=post_backup_check_data"
      post_backup_check_data:
        entrypoint: check
        quadlet_opt: "After=backup_data"
      forget_data:
        entrypoint: "forget --keep-daily 7 --keep-hourly 48 --prune"
        quadlet_opt: "After=post_backup_check_data"
      post_forget_check_data:
        entrypoint: check
        quadlet_opt: "After=forget_data"
  loop: "{{ lookup('ansible.builtin.dict', configs) }}"
  containers.podman.podman_container:
    name: "{{ item.key }}"
    image: ghcr.io/restic/restic
    state: quadlet
    hostname: "{{ inventory_hostname }}"
    secrets:
      - "wasabi_media_repo,type=env,target=RESTIC_REPOSITORY"
      - "restic_passwd_media,type=env,target=RESTIC_PASSWORD"
      - "wasabi_access_key_id,type=env,target=AWS_ACCESS_KEY_ID"
      - "wasabi_access_key,type=env,target=AWS_SECRET_ACCESS_KEY"
    volume: "{{ data_dir }}:/data:z,ro,noexec"
    entrypoint: "{{ item.value.entrypoint }}"
    quadlet_options:
      - |
        [Install]
        WantedBy=default.target
        {{ item.value.quadlet_opt }}

- name: Create the restic repo if it does not exist
  become: true
  become_user: "{{ username }}"
  register: restic_init_result
  failed_when: TODO Insert not failed if this repo already exists
  containers.podman.podman_container:
    image: ghcr.io/restic/restic
    name: init_data
    state: started
    secrets:
      - "wasabi_media_repo,type=env,target=RESTIC_REPOSITORY"
      - "restic_passwd_media,type=env,target=RESTIC_PASSWORD"
      - "wasabi_access_key_id,type=env,target=AWS_ACCESS_KEY_ID"
      - "wasabi_access_key,type=env,target=AWS_SECRET_ACCESS_KEY"
    entrypoint: "init"

- name: Generate script for data sync
  become: true
  become_user: root
  ansible.builtin.copy:
    dest: "~/sync.sh"
    group: "{{ username }}"
    owner: "{{ username }}"
    mode: '500'
    validate: bash -n %s
    content: |
      #!/usr/bin/bash
      set -euo pipefail
      export HISTCONTROL=ignorespace

      # copy does not propogate deletes from local to remote, sync does
      # bisync is experimental and propogates deletes from remote to local
      # copy for now while testing, consider sync if I start to accumulate garbage
      rclone copy {{ data_dir }} {{ backup_remote_name }}:nas-data -vv
